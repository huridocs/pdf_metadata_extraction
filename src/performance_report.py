import json
import pickle
import random
from os import listdir
from os.path import join
from pathlib import Path
from time import sleep, time

from paragraph_extraction_trainer.Paragraph import Paragraph
from paragraph_extraction_trainer.ParagraphExtractorTrainer import ParagraphExtractorTrainer
from paragraph_extraction_trainer.download_models import paragraph_extraction_model_path
from paragraph_extraction_trainer.model_configuration import MODEL_CONFIGURATION
from pdf_features.PdfFeatures import PdfFeatures
from pdf_tokens_type_trainer.ModelConfiguration import ModelConfiguration
from pdf_tokens_type_trainer.TokenTypeTrainer import TokenTypeTrainer
from sklearn.metrics import f1_score

from config import APP_PATH, ROOT_PATH
from data.ExtractionData import ExtractionData
from data.ExtractionIdentifier import ExtractionIdentifier
from data.LabeledData import LabeledData
from data.Option import Option
from data.PdfData import PdfData
from data.PredictionSample import PredictionSample
from data.TrainingSample import TrainingSample
from extractors.pdf_to_multi_option_extractor.PdfMultiOptionMethod import PdfMultiOptionMethod
from extractors.pdf_to_multi_option_extractor.PdfToMultiOptionExtractor import PdfToMultiOptionExtractor

PDF_MULTI_OPTION_EXTRACTION_LABELED_DATA_PATH = join(
    Path(__file__).parent, "extractors", "pdf_to_multi_option_extractor", "labeled_data"
)
PDF_DATA_FOLDER_PATH = join(ROOT_PATH, "data", "pdf_data_cache")
LABELED_DATA_PATH = join(APP_PATH, "pdf_topic_classification", "labeled_data")

LABELED_DATA_PDFS_PATH = join(ROOT_PATH.parent, "pdf-labeled-data", "pdfs")

BASE_LINE = {
    "cejil_president": (100.0, "NextWordsTokenSelectorFuzzy75"),
    "cyrilla_keywords": (53.49, "FuzzyFirstCleanLabel"),
    "cejil_date": (20.83, "FuzzyAll88"),
    "cejil_countries": (69.05, "FuzzyFirstCleanLabel"),
    "d4la_document_type": (44.07, "CleanBeginningDotDigits500_SingleLabelSetFit"),
    "cejil_secretary": (80.0, "FuzzyAll75"),
    "countries_in_favor": (99.75, "PreviousWordsSentenceSelectorFuzzyCommas"),
    "cejil_judge": (92.86, "FuzzyLast"),
}


def get_task_pdf_names():
    task_pdf_names: dict[str, set[str]] = dict()

    for task_name in listdir(str(PDF_MULTI_OPTION_EXTRACTION_LABELED_DATA_PATH)):
        with open(join(PDF_MULTI_OPTION_EXTRACTION_LABELED_DATA_PATH, task_name, "labels.json"), mode="r") as file:
            labels_dict: dict[str, list[str]] = json.load(file)
            task_pdf_names.setdefault(task_name, set()).update(labels_dict.keys())

    return task_pdf_names


def cache_pdf_data(pdf_name: str, pickle_path: Path):
    pdf_features = PdfFeatures.from_poppler_etree(join(LABELED_DATA_PDFS_PATH, pdf_name, "etree.xml"))

    trainer = TokenTypeTrainer([pdf_features], ModelConfiguration())
    trainer.set_token_types()
    trainer = ParagraphExtractorTrainer(pdfs_features=[pdf_features], model_configuration=MODEL_CONFIGURATION)
    paragraphs: list[Paragraph] = trainer.get_paragraphs(paragraph_extraction_model_path)

    pdf_data = PdfData(pdf_features, file_name=pdf_name)
    pdf_data.set_segments_from_paragraphs(paragraphs)

    with open(pickle_path, mode="wb") as file:
        pickle.dump(pdf_data, file)

    return pdf_data


def get_samples(task_name):
    with open(join(PDF_MULTI_OPTION_EXTRACTION_LABELED_DATA_PATH, task_name, "labels.json"), mode="r") as file:
        labels_dict: dict[str, list[str]] = json.load(file)

    multi_option_samples: list[TrainingSample] = list()
    for pdf_name in sorted(get_task_pdf_names()[task_name]):
        pickle_path = join(PDF_DATA_FOLDER_PATH, f"{pdf_name}.pickle")

        if Path(pickle_path).exists():
            with open(pickle_path, mode="rb") as file:
                pdf_data: PdfData = pickle.load(file)
        else:
            pdf_data: PdfData = cache_pdf_data(pdf_name, Path(pickle_path))

        values = [Option(id=x, label=x) for x in labels_dict[pdf_name]]
        language_iso = "es" if "cejil" in task_name else "en"

        extraction_sample = TrainingSample(
            pdf_data=pdf_data, labeled_data=LabeledData(values=values, language_iso=language_iso)
        )
        multi_option_samples.append(extraction_sample)

    random.seed(42)
    random.shuffle(multi_option_samples)
    return multi_option_samples


def get_multi_option_benchmark_data(filter_by: list[str] = None) -> list[ExtractionData]:
    benchmark_data: list[ExtractionData] = list()
    for task_name in listdir(str(PDF_MULTI_OPTION_EXTRACTION_LABELED_DATA_PATH)):
        if filter_by and task_name not in filter_by:
            continue

        print(f"Loading task {task_name}")

        with open(join(PDF_MULTI_OPTION_EXTRACTION_LABELED_DATA_PATH, task_name, "options.json"), mode="r") as file:
            options = [Option(id=x, label=x) for x in json.load(file)]

        multi_option_samples = get_samples(task_name)
        multi_value: bool = len([sample for sample in multi_option_samples if len(sample.labeled_data.values) > 1]) != 0
        extraction_identifier = ExtractionIdentifier(run_name="benchmark", extraction_name=task_name)
        benchmark_data.append(
            ExtractionData(
                samples=multi_option_samples,
                options=options,
                multi_value=multi_value,
                extraction_identifier=extraction_identifier,
            )
        )

    return benchmark_data


def performance_report():
    f1s_method_name = get_f1_scores_method_names()
    sleep(1)
    print()
    print("REPORT:")
    print("-------")
    for key, (value, method_name) in f1s_method_name.items():
        if value < BASE_LINE[key][0]:
            print(f"{key}: PERFORMANCE DECREASED!!!!!")
        else:
            print(f"{key}: Good performance")

        print(f"Base performance: {BASE_LINE[key][0]}% with method {BASE_LINE[key][1]}")
        print(f"Performance: {value}% with method {method_name}")
        print()


def get_f1_scores_method_names() -> dict[str, (float, str)]:
    f1s_method_name = dict()
    for dataset in get_multi_option_benchmark_data(filter_by=[]):
        truth_one_hot, prediction_one_hot, method_name, _ = get_predictions(dataset)
        f1 = round(100 * f1_score(truth_one_hot, prediction_one_hot, average="micro"), 2)
        f1s_method_name[dataset.extraction_identifier.extraction_name] = (f1, method_name)

    return f1s_method_name


def get_predictions(dataset: ExtractionData) -> (list[list[int]], list[list[int]], str):
    training_samples_number = int(len(dataset.samples) * 0.5) if len(dataset.samples) > 10 else 10
    training_samples = dataset.samples[:training_samples_number]
    test_samples = dataset.samples[training_samples_number:] if len(dataset.samples) > 20 else dataset.samples

    training_dataset = ExtractionData(
        samples=training_samples,
        options=dataset.options,
        multi_value=dataset.multi_value,
        extraction_identifier=dataset.extraction_identifier,
    )
    extractor = PdfToMultiOptionExtractor(dataset.extraction_identifier)
    extractor.create_model(training_dataset)
    prediction_samples = [PredictionSample(pdf_data=sample.pdf_data) for sample in test_samples]
    context_samples, predictions = extractor.get_predictions(prediction_samples)
    values_list = [x.labeled_data.values for x in test_samples]
    truth_one_hot = PdfMultiOptionMethod.one_hot_to_options_list(values_list, dataset.options)
    prediction_one_hot = PdfMultiOptionMethod.one_hot_to_options_list(predictions, dataset.options)
    return truth_one_hot, prediction_one_hot, extractor.get_best_method(training_dataset).get_name(), context_samples


def get_mistakes() -> dict[str, (float, str)]:
    f1s_method_name = dict()
    for dataset in get_multi_option_benchmark_data(filter_by=["cejil_president"]):
        truth_one_hot, prediction_one_hot, method_name, test_samples = get_predictions(dataset)

        correct = 0
        mistakes = 0
        for truth, prediction, sample in zip(truth_one_hot, prediction_one_hot, test_samples):
            text = " ".join([x.text_content for x in sample.pdf_data.pdf_data_segments if x.ml_label])
            missing = [dataset.options[i].label for i in range(len(truth)) if truth[i] and not prediction[i]]
            wrong = [dataset.options[i].label for i in range(len(truth)) if not truth[i] and prediction[i]]

            if missing or wrong:
                print()
                print(f"PDF: {sample.pdf_data.file_name}")
                print(f"Text: {text}")
                print(f"Missing: {missing}")
                print(f"Wrong: {wrong}")
                mistakes += 1
            else:
                correct += 1

        print(f"\n\nCorrect predictions for: {correct} PDFs")
        print(f"Incorrect predictions for {mistakes} PDFs")

    return f1s_method_name


if __name__ == "__main__":
    start = time()
    print("start")
    performance_report()
    # get_mistakes()
    print("time", round(time() - start, 2), "s")
